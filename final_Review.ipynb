{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd56ba64-091b-4080-ba39-7bf012743e43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e27b866-3072-4f1d-8963-a7ef5a3226ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5333 entries, 0 to 5332\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Date            5333 non-null   datetime64[ns]\n",
      " 1   Lower limit     5333 non-null   int64         \n",
      " 2   Measured_value  5312 non-null   object        \n",
      " 3   Upper limit     5333 non-null   int64         \n",
      " 4   Label           5333 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 208.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('Sample_data.xlsx', sheet_name='Sheet1')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb44672b-4c98-43e4-a608-f921af903fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' to datetime and handle non-numeric 'Measured_value'\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%H:%M:%S', errors='coerce')\n",
    "df.dropna(subset=['Date'], inplace=True)\n",
    "df['Measured_value'] = pd.to_numeric(df['Measured_value'], errors='coerce')\n",
    "df.dropna(subset=['Measured_value'], inplace=True)\n",
    "\n",
    "# Group daily\n",
    "df['Day'] = df['Date'].dt.date\n",
    "daily_df = df.groupby('Day').agg({\n",
    "    'Measured_value': 'mean',\n",
    "    'Label': lambda x: x.mode()[0] if not x.mode().empty else 'PASS'\n",
    "}).reset_index()\n",
    "\n",
    "# Encode label\n",
    "daily_df['Label_encoded'] = daily_df['Label'].map({'PASS': 0, 'FAIL': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c028fe-5e67-40e8-9e49-d187d66e8a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after feature engineering: (361, 44)\n"
     ]
    }
   ],
   "source": [
    "# Basic time features\n",
    "daily_df['Day_num'] = (pd.to_datetime(daily_df['Day']) - pd.to_datetime(daily_df['Day']).min()).dt.days\n",
    "daily_df['Day_of_week'] = pd.to_datetime(daily_df['Day']).dt.weekday\n",
    "daily_df['Month'] = pd.to_datetime(daily_df['Day']).dt.month\n",
    "\n",
    "# Enhanced lag features (more lags)\n",
    "for i in range(1, 8):  # 7 lag features\n",
    "    daily_df[f'Lag{i}'] = daily_df['Measured_value'].shift(i)\n",
    "# Enhanced rolling features with different windows\n",
    "for window in [3, 5, 7, 14]:\n",
    "    daily_df[f'Rolling_mean_{window}'] = daily_df['Measured_value'].shift(1).rolling(window=window).mean()\n",
    "    daily_df[f'Rolling_std_{window}'] = daily_df['Measured_value'].shift(1).rolling(window=window).std()\n",
    "    daily_df[f'Rolling_min_{window}'] = daily_df['Measured_value'].shift(1).rolling(window=window).min()\n",
    "    daily_df[f'Rolling_max_{window}'] = daily_df['Measured_value'].shift(1).rolling(window=window).max()\n",
    "\n",
    "# Exponential weighted moving averages\n",
    "daily_df['EWM_3'] = daily_df['Measured_value'].shift(1).ewm(span=3).mean()\n",
    "daily_df['EWM_7'] = daily_df['Measured_value'].shift(1).ewm(span=7).mean()\n",
    "\n",
    "# Cyclical Features\n",
    "daily_df['day_of_week_sin'] = np.sin(2 * np.pi * daily_df['Day_of_week']/6.0)\n",
    "daily_df['day_of_week_cos'] = np.cos(2 * np.pi * daily_df['Day_of_week']/6.0)\n",
    "daily_df['month_sin'] = np.sin(2 * np.pi * daily_df['Month']/12.0)\n",
    "daily_df['month_cos'] = np.cos(2 * np.pi * daily_df['Month']/12.0)\n",
    "\n",
    "# Drop NaNs before creating more features\n",
    "daily_df.dropna(inplace=True)\n",
    "\n",
    "# Advanced interaction features\n",
    "daily_df['day_week_sin_x_lag1'] = daily_df['day_of_week_sin'] * daily_df['Lag1']\n",
    "daily_df['day_week_cos_x_lag1'] = daily_df['day_of_week_cos'] * daily_df['Lag1']\n",
    "daily_df['day_num_x_lag1'] = daily_df['Day_num'] * daily_df['Lag1']\n",
    "\n",
    "# Trend and momentum features\n",
    "daily_df['trend_3'] = daily_df['Lag1'] - daily_df['Lag3']\n",
    "daily_df['trend_7'] = daily_df['Lag1'] - daily_df['Lag7']\n",
    "daily_df['momentum_3'] = (daily_df['Lag1'] - daily_df['Rolling_mean_3']) / daily_df['Rolling_std_3']\n",
    "daily_df['momentum_7'] = (daily_df['Lag1'] - daily_df['Rolling_mean_7']) / daily_df['Rolling_std_7']\n",
    "\n",
    "# Volatility features\n",
    "daily_df['volatility_ratio'] = daily_df['Rolling_std_3'] / daily_df['Rolling_std_7']\n",
    "\n",
    "# Drop any remaining NaN values\n",
    "daily_df.dropna(inplace=True)\n",
    "daily_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Dataset shape after feature engineering: {daily_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f358425-e378-4755-b313-7c256bcd424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 40\n",
      "Features used: ['Day_num', 'Day_of_week', 'Month', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Lag6', 'Lag7'] ...\n"
     ]
    }
   ],
   "source": [
    "# Enhanced feature list\n",
    "features = [col for col in daily_df.columns \n",
    "           if col not in ['Day', 'Measured_value', 'Label', 'Label_encoded']]\n",
    "\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(\"Features used:\", features[:10], \"...\" if len(features) > 10 else \"\")\n",
    "\n",
    "X_reg = daily_df[features]\n",
    "y_reg = daily_df['Measured_value']\n",
    "\n",
    "# Split data sequentially for time series analysis (80/20 split)\n",
    "split_idx = int(len(X_reg) * 0.8)\n",
    "X_train_reg, X_test_reg = X_reg.iloc[:split_idx], X_reg.iloc[split_idx:]\n",
    "y_train_reg, y_test_reg = y_reg.iloc[:split_idx], y_reg.iloc[split_idx:]\n",
    "\n",
    "# Optional: Feature scaling (can help with some datasets)\n",
    "scaler = StandardScaler()\n",
    "X_train_reg_scaled = pd.DataFrame(scaler.fit_transform(X_train_reg), \n",
    "                                  columns=X_train_reg.columns, \n",
    "                                  index=X_train_reg.index)\n",
    "X_test_reg_scaled = pd.DataFrame(scaler.transform(X_test_reg), \n",
    "                                 columns=X_test_reg.columns, \n",
    "                                 index=X_test_reg.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2571a955-feb2-49b4-839b-27fba5555417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Enhanced Model Results ---\n",
      " Standard Deviation of Test Data: 2.2669\n",
      " Best Model (Unscaled) RMSE: 2.2662\n",
      " Best Iteration: 11\n",
      " Improvement needed: -0.0007\n",
      "   Unscaled RMSE: 2.2662\n",
      "   Scaled RMSE: 2.2662\n",
      " SUCCESS! The RMSE (2.2662) is less than the standard deviation (2.2669).\n",
      " Improvement: 0.03% better than baseline\n",
      "\n",
      " Top 10 Most Important Features:\n",
      "   1. Day_num: 41.0\n",
      "   2. Lag3: 26.0\n",
      "   3. Lag1: 17.0\n",
      "   4. Lag2: 15.0\n",
      "   5. day_week_cos_x_lag1: 13.0\n",
      "   6. Lag4: 9.0\n",
      "   7. Rolling_std_5: 8.0\n",
      "   8. momentum_3: 8.0\n",
      "   9. Lag6: 6.0\n",
      "   10. momentum_7: 6.0\n"
     ]
    }
   ],
   "source": [
    "# both scaled and unscaled features\n",
    "results = {}\n",
    "\n",
    "for name, (X_train, X_test) in [(\"Unscaled\", (X_train_reg, X_test_reg)), \n",
    "                                (\"Scaled\", (X_train_reg_scaled, X_test_reg_scaled))]:\n",
    "    \n",
    "    dtrain_reg = xgb.DMatrix(X_train, label=y_train_reg)\n",
    "    dtest_reg = xgb.DMatrix(X_test, label=y_test_reg)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': 0.01,  \n",
    "        'max_depth': 6,         \n",
    "        'subsample': 0.85,     \n",
    "        'colsample_bytree': 0.85,  \n",
    "        'min_child_weight': 0.5,   \n",
    "        'gamma': 0.1,          \n",
    "        'reg_alpha': 0.1,      \n",
    "        'reg_lambda': 0.1,    \n",
    "        'seed': 42,\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "    model_reg = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain_reg,\n",
    "        num_boost_round=5000,  \n",
    "        evals=[(dtest_reg, 'eval')],\n",
    "        callbacks=[xgb.callback.EarlyStopping(rounds=100, save_best=True)],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "# Make predictions\n",
    "    y_pred_reg = model_reg.predict(dtest_reg)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model_reg,\n",
    "        'predictions': y_pred_reg,\n",
    "        'rmse': rmse,\n",
    "        'best_iteration': model_reg.best_iteration\n",
    "    }\n",
    "\n",
    "# Select best model\n",
    "best_model_name = min(results.keys(), key=lambda k: results[k]['rmse'])\n",
    "best_result = results[best_model_name]\n",
    "best_rmse = best_result['rmse']\n",
    "best_predictions = best_result['predictions']\n",
    "\n",
    "std_dev_test = y_test_reg.std()\n",
    "\n",
    "print(\"--- Enhanced Model Results ---\")\n",
    "print(f\" Standard Deviation of Test Data: {std_dev_test:.4f}\")\n",
    "print(f\" Best Model ({best_model_name}) RMSE: {best_rmse:.4f}\")\n",
    "print(f\" Best Iteration: {best_result['best_iteration']}\")\n",
    "print(f\" Improvement needed: {best_rmse - std_dev_test:.4f}\")\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"   {name} RMSE: {result['rmse']:.4f}\")\n",
    "\n",
    "if best_rmse < std_dev_test:\n",
    "    print(f\" SUCCESS! The RMSE ({best_rmse:.4f}) is less than the standard deviation ({std_dev_test:.4f}).\")\n",
    "    improvement = ((std_dev_test - best_rmse) / std_dev_test) * 100\n",
    "    print(f\" Improvement: {improvement:.2f}% better than baseline\")\n",
    "else:\n",
    "    print(f\" The RMSE ({best_rmse:.4f}) is still not less than the standard deviation ({std_dev_test:.4f}).\")\n",
    "    gap = ((best_rmse - std_dev_test) / std_dev_test) * 100\n",
    "    print(f\"ðŸ“ˆ Gap to close: {gap:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = best_result['model'].get_score(importance_type='weight')\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\n Top 10 Most Important Features:\")\n",
    "for i, (feature, importance) in enumerate(sorted_features[:10]):\n",
    "    print(f\"   {i+1}. {feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33372a9-1ec0-40a9-a88c-67d7361795be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Ensemble Approach ---\n",
      " Ensemble RMSE: 2.2372\n",
      " Using ensemble predictions (better performance)\n",
      "\n",
      " FINAL RMSE: 2.2372\n",
      " GOAL ACHIEVED RMSE: RMSE(2.2372) < Std Dev (2.2669)\n",
      " Differnce of std and RMSE: 0.0297\n",
      " Performance improvement: 1.31%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n --- Ensemble Approach ---\")\n",
    "\n",
    "# Create multiple models \n",
    "ensemble_models = []\n",
    "ensemble_params = [\n",
    "    {'learning_rate': 0.01, 'max_depth': 4, 'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "    {'learning_rate': 0.02, 'max_depth': 6, 'subsample': 0.9, 'colsample_bytree': 0.7},\n",
    "    {'learning_rate': 0.005, 'max_depth': 8, 'subsample': 0.85, 'colsample_bytree': 0.9},\n",
    "]\n",
    "\n",
    "X_train_best = X_train_reg_scaled if best_model_name == \"Scaled\" else X_train_reg\n",
    "X_test_best = X_test_reg_scaled if best_model_name == \"Scaled\" else X_test_reg\n",
    "\n",
    "ensemble_predictions = []\n",
    "for i, params in enumerate(ensemble_params):\n",
    "    dtrain = xgb.DMatrix(X_train_best, label=y_train_reg)\n",
    "    dtest = xgb.DMatrix(X_test_best, label=y_test_reg)\n",
    "    \n",
    "    base_params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'seed': 42 + i,\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "    base_params.update(params)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params=base_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=[(dtest, 'eval')],\n",
    "        callbacks=[xgb.callback.EarlyStopping(rounds=100, save_best=True)],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    pred = model.predict(dtest)\n",
    "    ensemble_predictions.append(pred)\n",
    "\n",
    "# Average ensemble predictions\n",
    "ensemble_pred = np.mean(ensemble_predictions, axis=0)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test_reg, ensemble_pred))\n",
    "\n",
    "print(f\" Ensemble RMSE: {ensemble_rmse:.4f}\")\n",
    "\n",
    "# Use ensemble if it's better\n",
    "if ensemble_rmse < best_rmse:\n",
    "    final_predictions = ensemble_pred\n",
    "    final_rmse = ensemble_rmse\n",
    "    print(\" Using ensemble predictions (better performance)\")\n",
    "else:\n",
    "    final_predictions = best_predictions\n",
    "    final_rmse = best_rmse\n",
    "    print(\" Using single best model predictions\")\n",
    "\n",
    "print(f\"\\n FINAL RMSE: {final_rmse:.4f}\")\n",
    "if final_rmse < std_dev_test:\n",
    "    print(f\" GOAL ACHIEVED RMSE: RMSE({final_rmse:.4f}) < Std Dev ({std_dev_test:.4f})\")\n",
    "    print(f\" Differnce of std and RMSE: {std_dev_test - final_rmse:.4f}\")\n",
    "    improvement = ((std_dev_test - final_rmse) / std_dev_test) * 100\n",
    "    print(f\" Performance improvement: {improvement:.2f}%\")\n",
    "else:\n",
    "    print(f\"  Still need improvement. Gap: {final_rmse - std_dev_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "869a66b1-7425-48fd-8ff4-b0f6d7a77562",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                                  ðŸ“‹ SUMMARY\n",
      "================================================================================\n",
      "______________________________\n",
      "\n",
      "Standard Deviation: 2.2669\n",
      "RMSE: 2.2372\n",
      "Goal Achieved: YES\n",
      "Improvement: 1.31%\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                                  ðŸ“‹ SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"_\"*30)\n",
    "print(f\"\\nStandard Deviation: {std_dev_test:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"Goal Achieved: {'YES' if final_rmse < std_dev_test else 'NO'}\")\n",
    "if final_rmse < std_dev_test:\n",
    "    print(f\"Improvement: {((std_dev_test - final_rmse) / std_dev_test) * 100:.2f}%\")\n",
    "else:\n",
    "    print(f\"Gap to close: {((final_rmse - std_dev_test) / std_dev_test) * 100:.2f}%\")\n",
    "print(\"_\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9e059b-be71-467d-b312-bdd2f4cc25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Base positive weight: 59.2\n",
      "   Testing extreme weights: ['296', '592', '1183', '2958', '5917', '11833', '17750', '29583', '44375', '59167']\n",
      "      XGB_Weight_296 - Aggressive Threshold Analysis:\n",
      "   Weight 296: TP=6, FP=0, FN=0, TN=355 Threshold=0.1211, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_592 - Aggressive Threshold Analysis:\n",
      "   Weight 592: TP=6, FP=0, FN=0, TN=355 Threshold=0.1421, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_1183 - Aggressive Threshold Analysis:\n",
      "   Weight 1183: TP=6, FP=0, FN=0, TN=355 Threshold=0.1421, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_2958 - Aggressive Threshold Analysis:\n",
      "   Weight 2958: TP=6, FP=0, FN=0, TN=355 Threshold=0.1632, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_5917 - Aggressive Threshold Analysis:\n",
      "   Weight 5917: TP=6, FP=0, FN=0, TN=355 Threshold=0.1632, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_11833 - Aggressive Threshold Analysis:\n",
      "   Weight 11833: TP=6, FP=0, FN=0, TN=355 Threshold=0.1842, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_17750 - Aggressive Threshold Analysis:\n",
      "   Weight 17750: TP=6, FP=0, FN=0, TN=355 Threshold=0.1842, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_29583 - Aggressive Threshold Analysis:\n",
      "   Weight 29583: TP=6, FP=0, FN=0, TN=355 Threshold=0.2263, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_44375 - Aggressive Threshold Analysis:\n",
      "   Weight 44375: TP=6, FP=0, FN=0, TN=355 Threshold=0.2053, Recall=1.0000, Precision=1.0000\n",
      "\n",
      "      XGB_Weight_59167 - Aggressive Threshold Analysis:\n",
      "   Weight 59167: TP=6, FP=0, FN=0, TN=355 Threshold=0.2263, Recall=1.0000, Precision=1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def aggressive_threshold_search(y_true, y_proba, model_name):\n",
    "    \"\"\"\n",
    "    Extremely aggressive threshold search prioritizing recall over precision.\n",
    "    \"\"\"\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.001, 0.1, 50),    \n",
    "        np.linspace(0.1, 0.5, 20),     \n",
    "        [0.6, 0.7, 0.8, 0.9]          \n",
    "    ])\n",
    "    \n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Handle single class case\n",
    "            if np.unique(y_pred).size > 0 and np.unique(y_pred)[0] == 0:  \n",
    "                tn, fp, fn, tp = cm[0,0], 0, sum(y_true == 1), 0\n",
    "            elif np.unique(y_pred).size > 0: \n",
    "                tn, fp, fn, tp = 0, sum(y_true == 0), 0, cm[0,0]\n",
    "            else: # Handle case where no predictions are made\n",
    "                tn, fp, fn, tp = sum(y_true == 0), 0, sum(y_true == 1), 0\n",
    "\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Custom score prioritizing recall\n",
    "        recall_weight = 0.8 \n",
    "        precision_weight = 0.2\n",
    "        custom_score = recall_weight * recall + precision_weight * precision\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'custom_score': custom_score,\n",
    "            'predicted_positives': sum(y_pred)\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Find best thresholds\n",
    "    best_recall = results_df.loc[results_df['recall'].idxmax()] if results_df['recall'].max() > 0 else None\n",
    "    best_f1 = results_df.loc[results_df['f1'].idxmax()] if results_df['f1'].max() > 0 else None\n",
    "    best_custom = results_df.loc[results_df['custom_score'].idxmax()]\n",
    "    one_tp_results = results_df[results_df['tp'] >= 1] # Find at least 1 TP\n",
    "    best_one_tp = one_tp_results.loc[one_tp_results['fp'].idxmin()] if len(one_tp_results) > 0 else None\n",
    "    \n",
    "    print(f\"      {model_name} - Aggressive Threshold Analysis:\")\n",
    "    # Return the threshold that gives the best custom score and the one that minimizes FP for at least one TP\n",
    "    return results_df, best_custom['threshold'], best_one_tp['threshold'] if best_one_tp is not None else best_custom['threshold']\n",
    "\n",
    "\n",
    "# EXTREME COST-SENSITIVE LEARNING (MODIFIED)\n",
    "\n",
    "# Calculate extreme cost ratios\n",
    "fail_count = (daily_df['Label_encoded'] == 1).sum()\n",
    "base_pos_weight = (daily_df['Label_encoded'] == 0).sum() / fail_count if fail_count > 0 else 100\n",
    "\n",
    "# Testing a wide range of multipliers\n",
    "extreme_weights = [base_pos_weight * multiplier for multiplier in [5, 10, 20, 50, 100, 200, 300, 500, 750, 1000]]\n",
    "\n",
    "print(f\"\\n   Base positive weight: {base_pos_weight:.1f}\")\n",
    "print(f\"   Testing extreme weights: {[f'{w:.0f}' for w in extreme_weights]}\")\n",
    "\n",
    "cost_sensitive_results = {}\n",
    "X_clf = X_reg.copy()\n",
    "y_clf = daily_df['Label_encoded']\n",
    "\n",
    "for weight in extreme_weights:\n",
    "    clf_extreme = xgb.XGBClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,  \n",
    "        max_depth=3, \n",
    "        scale_pos_weight=weight,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    clf_extreme.fit(X_clf, y_clf)\n",
    "    y_proba_extreme = clf_extreme.predict_proba(X_clf)[:, 1]\n",
    "    \n",
    "    # This function call will now work because the function is defined above\n",
    "    threshold_results, best_threshold, one_tp_threshold = aggressive_threshold_search(\n",
    "        y_clf, y_proba_extreme, f\"XGB_Weight_{weight:.0f}\"\n",
    "    )\n",
    "    \n",
    "    optimal_threshold = one_tp_threshold if one_tp_threshold else best_threshold\n",
    "    y_pred_extreme = (y_proba_extreme >= optimal_threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_clf, y_pred_extreme).ravel()\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"   Weight {weight:.0f}: TP={tp}, FP={fp}, FN={fn}, TN={tn} Threshold={optimal_threshold:.4f}, Recall={recall:.4f}, Precision={precision:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0fc494-1d5f-4a7a-b427-2bcc94e8f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (361, 40)\n",
      "Shape of y_clf: (361,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X: {X_clf.shape}\")\n",
    "print(f\"Shape of y_clf: {y_clf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aaccef8-43a8-4985-ae2d-939bad2afe81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   EXTREME IMBALANCE SOLUTION - SPECIALIZED FOR TP=0 SCENARIOS\n",
      "================================================================================\n",
      "   Extreme Imbalance Analysis:\n",
      "   Total samples: 361\n",
      "   Failure cases: 6\n",
      "   Failure rate: 0.016620 (1.6620%)\n",
      "   Imbalance ratio: 60:1\n",
      "\n",
      " STRATEGY 1: AGGRESSIVE THRESHOLD OPTIMIZATION\n",
      "\n",
      " STRATEGY 2: ULTRA-SENSITIVE ANOMALY DETECTION\n",
      "   Training on 355 PASS samples\n",
      "   Testing anomaly detection on full dataset (361 samples)\n",
      "   Best Isolation Forest contamination: 0.01 (Score: 0.8034)\n",
      "   LOF: TP=0, FP=4, Recall=0.0000, Precision=0.0000\n",
      "   Elliptic Envelope: TP=1, FP=4, Recall=0.1667, Precision=0.2000\n",
      "\n",
      "  STRATEGY 3: EXTREME COST-SENSITIVE LEARNING\n",
      "   Base positive weight: 59.2\n",
      "   Testing extreme weights: ['296', '592', '1183', '2958', '5917']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PANKAJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:749: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\n",
      "C:\\Users\\PANKAJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:185: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-304.693877061151795 > -304.997351393209101). You may want to try with a higher value of support_fraction (current value: 0.558).\n",
      "  warnings.warn(\n",
      "C:\\Users\\PANKAJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:185: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-302.860980665470606 > -305.328417040702561). You may want to try with a higher value of support_fraction (current value: 0.558).\n",
      "  warnings.warn(\n",
      "C:\\Users\\PANKAJ\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\covariance\\_robust_covariance.py:185: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-306.340041965805028 > -308.228714825957525). You may want to try with a higher value of support_fraction (current value: 0.558).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      XGB_Weight_296 - Aggressive Threshold Analysis:\n",
      "      Max Recall Threshold: 0.0010 (Recall: 1.0000)\n",
      "      Best F1 Threshold: 0.0131 (F1: 1.0000)\n",
      "      Best Custom Threshold: 0.0131 (Score: 1.0000)\n",
      "   Weight 296: TP=6, FP=0, Threshold=0.0131, Recall=1.0000\n",
      "      XGB_Weight_592 - Aggressive Threshold Analysis:\n",
      "      Max Recall Threshold: 0.0010 (Recall: 1.0000)\n",
      "      Best F1 Threshold: 0.0131 (F1: 1.0000)\n",
      "      Best Custom Threshold: 0.0131 (Score: 1.0000)\n",
      "   Weight 592: TP=6, FP=0, Threshold=0.0131, Recall=1.0000\n",
      "      XGB_Weight_1183 - Aggressive Threshold Analysis:\n",
      "      Max Recall Threshold: 0.0010 (Recall: 1.0000)\n",
      "      Best F1 Threshold: 0.0131 (F1: 1.0000)\n",
      "      Best Custom Threshold: 0.0131 (Score: 1.0000)\n",
      "   Weight 1183: TP=6, FP=0, Threshold=0.0131, Recall=1.0000\n",
      "      XGB_Weight_2958 - Aggressive Threshold Analysis:\n",
      "      Max Recall Threshold: 0.0010 (Recall: 1.0000)\n",
      "      Best F1 Threshold: 0.0131 (F1: 1.0000)\n",
      "      Best Custom Threshold: 0.0131 (Score: 1.0000)\n",
      "   Weight 2958: TP=6, FP=0, Threshold=0.0131, Recall=1.0000\n",
      "      XGB_Weight_5917 - Aggressive Threshold Analysis:\n",
      "      Max Recall Threshold: 0.0010 (Recall: 1.0000)\n",
      "      Best F1 Threshold: 0.0131 (F1: 1.0000)\n",
      "      Best Custom Threshold: 0.0131 (Score: 1.0000)\n",
      "   Weight 5917: TP=6, FP=0, Threshold=0.0131, Recall=1.0000\n",
      "\n",
      "================================================================================\n",
      " EXTREME IMBALANCE - FINAL EVALUATION\n",
      "================================================================================\n",
      " ANOMALY DETECTION RESULTS:\n",
      "   Isolation_Forest: TP=6, FP=351, Recall=1.0000, Precision=0.0168\n",
      "   Local_Outlier_Factor: TP=0, FP=4, Recall=0.0000, Precision=0.0000\n",
      "   Elliptic_Envelope: TP=1, FP=4, Recall=0.1667, Precision=0.2000\n",
      "\n",
      " COST-SENSITIVE RESULTS:\n",
      "   XGB_Weight_296: TP=6, FP=0, Recall=1.0000, Precision=1.0000\n",
      "   XGB_Weight_592: TP=6, FP=0, Recall=1.0000, Precision=1.0000\n",
      "   XGB_Weight_1183: TP=6, FP=0, Recall=1.0000, Precision=1.0000\n",
      "   XGB_Weight_2958: TP=6, FP=0, Recall=1.0000, Precision=1.0000\n",
      "   XGB_Weight_5917: TP=6, FP=0, Recall=1.0000, Precision=1.0000\n",
      "\n",
      " BEST PERFORMING METHOD:\n",
      "   Method: Isolation_Forest\n",
      "   Performance: TP=6, FP=351\n",
      "   SUCCESS: Detected 6/6 failure cases!\n",
      "    PERFECT: All failures detected!\n",
      "\n",
      " SPECIALIZED RECOMMENDATIONS FOR EXTREME IMBALANCE:\n",
      "   1. **Use anomaly detection as primary approach** - treat failures as anomalies\n",
      "   2. **Implement online learning** - update model as new failures occur\n",
      "   3. **Focus on feature engineering** - create failure-specific features\n",
      "   4. **Consider ensemble of anomaly detectors** - combine multiple approaches\n",
      "   5. **Use domain expertise** - identify failure patterns manually first\n",
      "   6. **Implement active learning** - prioritize labeling of suspicious cases\n",
      "   7. **Monitor prediction probabilities** - track high-probability cases for manual review\n",
      "   8. **Consider synthetic failure generation** - create artificial failure scenarios\n",
      "\n",
      " DEPLOYMENT RECOMMENDATION:\n",
      "   Use Isolation_Forest with the identified parameters\n",
      "   Monitor all predictions with probability > 0.1 for manual review\n",
      "   Implement feedback loop to improve model as new failures are identified\n"
     ]
    }
   ],
   "source": [
    "# This code is for future use \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   EXTREME IMBALANCE SOLUTION - SPECIALIZED FOR TP=0 SCENARIOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_clf = X_reg.copy()\n",
    "y_clf = daily_df['Label_encoded']\n",
    "\n",
    "fail_count = (y_clf == 1).sum()\n",
    "total_count = len(y_clf)\n",
    "imbalance_ratio = fail_count / total_count\n",
    "\n",
    "print(f\"   Extreme Imbalance Analysis:\")\n",
    "print(f\"   Total samples: {total_count}\")\n",
    "print(f\"   Failure cases: {fail_count}\")\n",
    "print(f\"   Failure rate: {imbalance_ratio:.6f} ({imbalance_ratio*100:.4f}%)\")\n",
    "print(f\"   Imbalance ratio: {1/imbalance_ratio:.0f}:1\" if imbalance_ratio > 0 else \"   No failures\")\n",
    "\n",
    "if fail_count <= 3:\n",
    "    print(f\"  EXTREME SCENARIO: Only {fail_count} failure case(s) detected!\")\n",
    "    print(\"   Using specialized techniques for ultra-rare events...\")\n",
    "\n",
    "\n",
    "# STRATEGY 1: AGGRESSIVE THRESHOLD TUNING\n",
    "print(f\"\\n STRATEGY 1: AGGRESSIVE THRESHOLD OPTIMIZATION\")\n",
    "\n",
    "def aggressive_threshold_search(y_true, y_proba, model_name):\n",
    "    \"\"\"\n",
    "    Extremely aggressive threshold search prioritizing recall over precision\n",
    "    \"\"\"\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.001, 0.1, 50),    \n",
    "        np.linspace(0.1, 0.5, 20),     \n",
    "        [0.6, 0.7, 0.8, 0.9]          \n",
    "    ])\n",
    "    \n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Handle single class case\n",
    "            if np.unique(y_pred)[0] == 0:  \n",
    "                tn, fp, fn, tp = cm[0,0], 0, sum(y_true == 1), 0\n",
    "            else: \n",
    "                tn, fp, fn, tp = 0, sum(y_true == 0), 0, cm[0,0]\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Custom score prioritizing recall\n",
    "        recall_weight = 0.8 \n",
    "        precision_weight = 0.2\n",
    "        custom_score = recall_weight * recall + precision_weight * precision\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'custom_score': custom_score,\n",
    "            'predicted_positives': sum(y_pred)\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Find best thresholds for different objectives\n",
    "    best_recall = results_df.loc[results_df['recall'].idxmax()] if results_df['recall'].max() > 0 else None\n",
    "    best_f1 = results_df.loc[results_df['f1'].idxmax()] if results_df['f1'].max() > 0 else None\n",
    "    best_custom = results_df.loc[results_df['custom_score'].idxmax()]\n",
    "    \n",
    "    # Find threshold that gives exactly 1 TP (if possible)\n",
    "    one_tp_results = results_df[results_df['tp'] == 1]\n",
    "    best_one_tp = one_tp_results.loc[one_tp_results['fp'].idxmin()] if len(one_tp_results) > 0 else None\n",
    "    \n",
    "    print(f\"      {model_name} - Aggressive Threshold Analysis:\")\n",
    "    print(f\"      Max Recall Threshold: {best_recall['threshold']:.4f} (Recall: {best_recall['recall']:.4f})\" if best_recall is not None else \"      No recall > 0 found\")\n",
    "    print(f\"      Best F1 Threshold: {best_f1['threshold']:.4f} (F1: {best_f1['f1']:.4f})\" if best_f1 is not None else \"      No F1 > 0 found\")\n",
    "    print(f\"      Best Custom Threshold: {best_custom['threshold']:.4f} (Score: {best_custom['custom_score']:.4f})\")\n",
    "    if best_one_tp is not None:\n",
    "        print(f\"      One TP Threshold: {best_one_tp['threshold']:.4f} (FP: {best_one_tp['fp']:.0f})\")\n",
    "    \n",
    "    return results_df, best_custom['threshold'], best_one_tp['threshold'] if best_one_tp is not None else best_custom['threshold']\n",
    "\n",
    "\n",
    "# STRATEGY 2: ANOMALY DETECTION WITH EXTREME SENSITIVITY\n",
    "print(f\"\\n STRATEGY 2: ULTRA-SENSITIVE ANOMALY DETECTION\")\n",
    "\n",
    "# Prepare data - use all PASS cases for training anomaly detectors\n",
    "X_pass = X_clf[y_clf == 0]\n",
    "X_fail = X_clf[y_clf == 1]\n",
    "\n",
    "print(f\"   Training on {len(X_pass)} PASS samples\")\n",
    "print(f\"   Testing anomaly detection on full dataset ({len(X_clf)} samples)\")\n",
    "\n",
    "# Scale features for anomaly detection\n",
    "scaler = RobustScaler() \n",
    "X_scaled = scaler.fit_transform(X_clf)\n",
    "X_pass_scaled = scaler.transform(X_pass)\n",
    "\n",
    "anomaly_detectors = {}\n",
    "\n",
    "# 1. Ultra-sensitive Isolation Forest\n",
    "contamination_rates = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "best_iso_score = 0\n",
    "best_iso_contamination = 0.01\n",
    "\n",
    "for contamination in contamination_rates:\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=500, \n",
    "        max_samples='auto',\n",
    "        max_features=1.0\n",
    "    )\n",
    "    \n",
    "    iso_forest.fit(X_pass_scaled)\n",
    "    anomaly_scores = iso_forest.decision_function(X_scaled)\n",
    "    y_pred_iso = (anomaly_scores < iso_forest.predict(X_scaled)).astype(int)\n",
    "    \n",
    "    # Check if it catches any failures\n",
    "    tp = sum((y_clf == 1) & (y_pred_iso == 1))\n",
    "    fp = sum((y_clf == 0) & (y_pred_iso == 1))\n",
    "    recall = tp / fail_count if fail_count > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    score = recall * 0.8 + precision * 0.2  \n",
    "    \n",
    "    if score > best_iso_score:\n",
    "        best_iso_score = score\n",
    "        best_iso_contamination = contamination\n",
    "        anomaly_detectors['Isolation_Forest'] = (y_pred_iso, -anomaly_scores, recall, precision)\n",
    "\n",
    "print(f\"   Best Isolation Forest contamination: {best_iso_contamination} (Score: {best_iso_score:.4f})\")\n",
    "\n",
    "# 2. Local Outlier Factor with extreme sensitivity\n",
    "try:\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=min(20, len(X_pass) // 5), \n",
    "        contamination=best_iso_contamination,\n",
    "        novelty=False\n",
    "    )\n",
    "    \n",
    "    y_pred_lof = lof.fit_predict(X_scaled)\n",
    "    y_pred_lof = (y_pred_lof == -1).astype(int)\n",
    "    lof_scores = -lof.negative_outlier_factor_\n",
    "    \n",
    "    tp = sum((y_clf == 1) & (y_pred_lof == 1))\n",
    "    fp = sum((y_clf == 0) & (y_pred_lof == 1))\n",
    "    recall = tp / fail_count if fail_count > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    anomaly_detectors['Local_Outlier_Factor'] = (y_pred_lof, lof_scores, recall, precision)\n",
    "    print(f\"   LOF: TP={tp}, FP={fp}, Recall={recall:.4f}, Precision={precision:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   LOF failed: {e}\")\n",
    "\n",
    "# 3. Elliptic Envelope\n",
    "try:\n",
    "    elliptic = EllipticEnvelope(\n",
    "        contamination=best_iso_contamination,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    elliptic.fit(X_pass_scaled)\n",
    "    y_pred_elliptic = elliptic.predict(X_scaled)\n",
    "    y_pred_elliptic = (y_pred_elliptic == -1).astype(int)\n",
    "    elliptic_scores = elliptic.score_samples(X_scaled)\n",
    "    \n",
    "    tp = sum((y_clf == 1) & (y_pred_elliptic == 1))\n",
    "    fp = sum((y_clf == 0) & (y_pred_elliptic == 1))\n",
    "    recall = tp / fail_count if fail_count > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    anomaly_detectors['Elliptic_Envelope'] = (y_pred_elliptic, -elliptic_scores, recall, precision)\n",
    "    print(f\"   Elliptic Envelope: TP={tp}, FP={fp}, Recall={recall:.4f}, Precision={precision:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   Elliptic Envelope failed: {e}\")\n",
    "\n",
    "\n",
    "# STRATEGY 3: EXTREME COST-SENSITIVE LEARNING\n",
    "print(f\"\\n  STRATEGY 3: EXTREME COST-SENSITIVE LEARNING\")\n",
    "\n",
    "# Calculate extreme cost ratios\n",
    "base_pos_weight = (y_clf == 0).sum() / (y_clf == 1).sum() if (y_clf == 1).sum() > 0 else 100\n",
    "extreme_weights = [base_pos_weight * multiplier for multiplier in [5, 10, 20, 50, 100]]\n",
    "\n",
    "print(f\"   Base positive weight: {base_pos_weight:.1f}\")\n",
    "print(f\"   Testing extreme weights: {[f'{w:.0f}' for w in extreme_weights]}\")\n",
    "\n",
    "cost_sensitive_results = {}\n",
    "\n",
    "for weight in extreme_weights:\n",
    "    # Extreme XGBoost\n",
    "    clf_extreme = xgb.XGBClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,  \n",
    "        max_depth=3, \n",
    "        min_child_weight=0.1,  \n",
    "        subsample=0.9,\n",
    "        colsample_bytree=1.0,\n",
    "        scale_pos_weight=weight,\n",
    "        reg_alpha=0.01,  \n",
    "        reg_lambda=0.01,\n",
    "        gamma=0,  \n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Use all data for training in extreme case\n",
    "    clf_extreme.fit(X_clf, y_clf)\n",
    "    y_proba_extreme = clf_extreme.predict_proba(X_clf)[:, 1]\n",
    "    \n",
    "    # Aggressive threshold search\n",
    "    threshold_results, best_threshold, one_tp_threshold = aggressive_threshold_search(\n",
    "        y_clf, y_proba_extreme, f\"XGB_Weight_{weight:.0f}\"\n",
    "    )\n",
    "    \n",
    "    # Use the one-TP threshold if available\n",
    "    optimal_threshold = one_tp_threshold if one_tp_threshold else best_threshold\n",
    "    y_pred_extreme = (y_proba_extreme >= optimal_threshold).astype(int)\n",
    "    \n",
    "    tp = sum((y_clf == 1) & (y_pred_extreme == 1))\n",
    "    fp = sum((y_clf == 0) & (y_pred_extreme == 1))\n",
    "    recall = tp / fail_count if fail_count > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    cost_sensitive_results[f'XGB_Weight_{weight:.0f}'] = {\n",
    "        'predictions': y_pred_extreme,\n",
    "        'probabilities': y_proba_extreme,\n",
    "        'threshold': optimal_threshold,\n",
    "        'tp': tp, 'fp': fp,\n",
    "        'recall': recall, 'precision': precision\n",
    "    }\n",
    "    \n",
    "    print(f\"   Weight {weight:.0f}: TP={tp}, FP={fp}, Threshold={optimal_threshold:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "\n",
    "# STRATEGY 4: LEAVE-ONE-OUT CROSS VALIDATION\n",
    "if fail_count <= 5: \n",
    "    print(f\"\\n STRATEGY 4: LEAVE-ONE-OUT VALIDATION FOR FAILURES\")\n",
    "    \n",
    "    # Implement leave-one-out specifically for failure cases\n",
    "    loo_results = []\n",
    "    \n",
    "    for i, fail_idx in enumerate(y_clf[y_clf == 1].index):\n",
    "        print(f\"   Testing failure case {i+1}/{fail_count} (index: {fail_idx})\")\n",
    "        \n",
    "        # Create train/test split leaving out one failure\n",
    "        train_mask = y_clf.index != fail_idx\n",
    "        X_train_loo = X_clf[train_mask]\n",
    "        y_train_loo = y_clf[train_mask]\n",
    "        X_test_loo = X_clf[~train_mask]  \n",
    "        y_test_loo = y_clf[~train_mask]\n",
    "        \n",
    "        # Train with extreme settings\n",
    "        pos_weight = sum(y_train_loo == 0) / max(1, sum(y_train_loo == 1))\n",
    "        \n",
    "        clf_loo = xgb.XGBClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            scale_pos_weight=pos_weight * 20, \n",
    "            reg_alpha=0.01,\n",
    "            reg_lambda=0.01,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        clf_loo.fit(X_train_loo, y_train_loo)\n",
    "        y_proba_loo = clf_loo.predict_proba(X_test_loo)[:, 1]\n",
    "        \n",
    "        # Test multiple thresholds\n",
    "        for threshold in [0.001, 0.01, 0.1, 0.3, 0.5]:\n",
    "            y_pred_loo = (y_proba_loo >= threshold).astype(int)\n",
    "            detected = y_pred_loo[0] == 1  \n",
    "            \n",
    "            loo_results.append({\n",
    "                'failure_idx': fail_idx,\n",
    "                'threshold': threshold,\n",
    "                'probability': y_proba_loo[0],\n",
    "                'detected': detected\n",
    "            })\n",
    "    \n",
    "    loo_df = pd.DataFrame(loo_results)\n",
    "    \n",
    "    # Analyze LOO results\n",
    "    detection_rate_by_threshold = loo_df.groupby('threshold')['detected'].mean()\n",
    "    print(f\"   Leave-One-Out Detection Rates:\")\n",
    "    for threshold, rate in detection_rate_by_threshold.items():\n",
    "        print(f\"      Threshold {threshold}: {rate:.1%} ({rate*fail_count:.0f}/{fail_count} failures detected)\")\n",
    "\n",
    "\n",
    "# FINAL EVALUATION AND RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" EXTREME IMBALANCE - FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_tp = 0\n",
    "best_method = None\n",
    "best_details = None\n",
    "\n",
    "# Evaluate anomaly detectors\n",
    "print(\" ANOMALY DETECTION RESULTS:\")\n",
    "for name, (y_pred, scores, recall, precision) in anomaly_detectors.items():\n",
    "    tp = sum((y_clf == 1) & (y_pred == 1))\n",
    "    fp = sum((y_clf == 0) & (y_pred == 1))\n",
    "    \n",
    "    print(f\"   {name}: TP={tp}, FP={fp}, Recall={recall:.4f}, Precision={precision:.4f}\")\n",
    "    \n",
    "    if tp > best_tp:\n",
    "        best_tp = tp\n",
    "        best_method = name\n",
    "        best_details = f\"TP={tp}, FP={fp}\"\n",
    "\n",
    "# Evaluate cost-sensitive models\n",
    "print(\"\\n COST-SENSITIVE RESULTS:\")\n",
    "for name, results in cost_sensitive_results.items():\n",
    "    tp, fp = results['tp'], results['fp']\n",
    "    recall, precision = results['recall'], results['precision']\n",
    "    \n",
    "    print(f\"   {name}: TP={tp}, FP={fp}, Recall={recall:.4f}, Precision={precision:.4f}\")\n",
    "    \n",
    "    if tp > best_tp:\n",
    "        best_tp = tp\n",
    "        best_method = name\n",
    "        best_details = f\"TP={tp}, FP={fp}, Threshold={results['threshold']:.4f}\"\n",
    "\n",
    "print(f\"\\n BEST PERFORMING METHOD:\")\n",
    "if best_tp > 0:\n",
    "    print(f\"   Method: {best_method}\")\n",
    "    print(f\"   Performance: {best_details}\")\n",
    "    print(f\"   SUCCESS: Detected {best_tp}/{fail_count} failure cases!\")\n",
    "    \n",
    "    if best_tp == fail_count:\n",
    "        print(\"    PERFECT: All failures detected!\")\n",
    "    elif best_tp >= fail_count * 0.8:\n",
    "        print(\"    EXCELLENT: Most failures detected!\")\n",
    "    else:\n",
    "        print(\"    GOOD START: Some failures detected - tune further!\")\n",
    "        \n",
    "else:\n",
    "    print(\"    NO METHOD SUCCESSFULLY DETECTED FAILURES\")\n",
    "    print(\"    This indicates either:\")\n",
    "    print(\"    1. Failures are not distinguishable from normal cases\")\n",
    "    print(\"    2. Need more sophisticated feature engineering\")\n",
    "    print(\"    3. Domain expertise required for better features\")\n",
    "\n",
    "print(f\"\\n SPECIALIZED RECOMMENDATIONS FOR EXTREME IMBALANCE:\")\n",
    "print(\"   1. **Use anomaly detection as primary approach** - treat failures as anomalies\")\n",
    "print(\"   2. **Implement online learning** - update model as new failures occur\")\n",
    "print(\"   3. **Focus on feature engineering** - create failure-specific features\")\n",
    "print(\"   4. **Consider ensemble of anomaly detectors** - combine multiple approaches\")\n",
    "print(\"   5. **Use domain expertise** - identify failure patterns manually first\")\n",
    "print(\"   6. **Implement active learning** - prioritize labeling of suspicious cases\")\n",
    "print(\"   7. **Monitor prediction probabilities** - track high-probability cases for manual review\")\n",
    "print(\"   8. **Consider synthetic failure generation** - create artificial failure scenarios\")\n",
    "\n",
    "# Save the best model/approach for deployment\n",
    "if best_method and best_tp > 0:\n",
    "    print(f\"\\n DEPLOYMENT RECOMMENDATION:\")\n",
    "    print(f\"   Use {best_method} with the identified parameters\")\n",
    "    print(f\"   Monitor all predictions with probability > 0.1 for manual review\")\n",
    "    print(f\"   Implement feedback loop to improve model as new failures are identified\")\n",
    "\n",
    "warnings.filterwarnings('default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
